#import libraries
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Initialize the tokenizer and model from the DialoGPT-medium pretrained model
tokenizer = GPT2Tokenizer.from_pretrained("microsoft/DialoGPT-medium")
model = GPT2LMHeadModel.from_pretrained("microsoft/DialoGPT-medium")

def run_bot(user_text, chat_history_ids):
    """
    This function generates a response from a chatbot model based on the user's input text and previous chat history.

    Args:
        user_text (str): The text input provided by the user.
        chat_history_ids (torch.Tensor or None): The chat history encoded as input IDs, or None if no history exists.

    Returns:
        str: The response generated by the chatbot model based on the user's input and chat history.
        torch.Tensor: The updated chat history after incorporating the user's input and generated response.
    """
    
    # Tokenize the user's text and append the end-of-sequence token
    input_ids = tokenizer.encode(user_text + tokenizer.eos_token, return_tensors="pt")
    
     # Concatenate the chat history with the user's input, if available
    if chat_history_ids is None:
        bot_input_ids = input_ids
    else:
        bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1)
        
    # Generate a response from the model based on the concatenated input
    chat_history_ids = model.generate(bot_input_ids, max_length=1000,pad_token_id=tokenizer.eos_token_id)
    
    # Decode the generated response and remove special tokens for display
    resp = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0],skip_special_tokens=True)
    
    # Return the decoded response and updated chat history
    return resp, chat_history_ids

